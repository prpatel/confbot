spring.application.name=confbot
server.port=8888

spring.datasource.url=jdbc:postgresql://localhost:5432/conference_db
spring.datasource.username=conference_user
spring.datasource.password=conference_pass
spring.jpa.hibernate.ddl-auto=update

#logging.level.org.springframework.ai.chat.client.advisor=DEBUG

spring.ai.model.embedding=ollama
spring.ai.vectorstore.pgvector.index-type=HNSW
spring.ai.vectorstore.pgvector.distance-type=COSINE_DISTANCE
spring.ai.vectorstore.pgvector.dimensions=768
spring.ai.ollama.embedding.options.model=nomic-embed-text
spring.ai.vectorstore.pgvector.initialize-schema=true

# When multiple model starters are present, Spring Boot's auto-configuration attempts to create a default ChatClient.Builder.
# Because both starters provide a ChatModel bean, the framework cannot determine which model should back the default client,
# leading to a NoUniqueBeanDefinitionException.
# Hence, we disable auto LLM Chat configuration as we're going to use two different "providers",
# OpenAI/LM Studio and Ollama
# These are defined in the AiConfig class
spring.ai.chat.client.enabled=false

# PC with 5090, able to run models under 32GB
spring.ai.ollama.base-url=http://100.119.211.80:11434
#spring.ai.ollama.chat.model=nemotron-3-nano
spring.ai.ollama.chat.model=qwen3-coder:latest

#spring.ai.ollama.chat.model=qwen3-coder:latest
#spring.ai.ollama.chat.model=nemotron-3-nano:latest
#spring.ai.ollama.chat.model=olmo-3.1:32b-instruct
#spring.ai.ollama.chat.model=glm-4.7-flash:latest
#spring.ai.ollama.chat.options.max-tokens=900000


# This is where you can specify your own LM Studio connection, or use your OpenAI key and comment out
# the base-url as it will default to the standard OpenAI endpoint.
# You still need to provide a key for OpenAI, and the best practice is to pass it in via ENV as below
# spring.ai.openai.api-key=${OPEN_AI_API_KEY}

###### lm studio running locally
#spring.ai.openai.base-url=http://localhost:1234
#spring.ai.openai.api-key=sk-lm-JqXTLoid:a0PyO14ouenpcLrk4VsG

#spring.ai.openai.chat.options.model=zai-org/glm-4.7-flash
#spring.ai.openai.chat.options.model=nvidia/nemotron-3-nano
#spring.ai.openai.chat.options.model=nvidia-nemotron-3-nano-30b-a3b-mlx@8bit
# gemma-3-27b: good model with accuracy, but slow
#spring.ai.openai.chat.options.model=google/gemma-3-27b

#spring.ai.openai.chat.options.model=qwen/qwen3-coder-30b

###### My Mac Studio for remote model execution, running MLX accelerated models
spring.ai.openai.base-url=http://100.90.81.40:1234/
spring.ai.openai.api-key=sk-lm-QXPHucXX:X11pReoRoT6QPwUOcWU3

# qwen3-coder-next : currently the best in terms of accuracy and speed
spring.ai.openai.chat.options.model=qwen/qwen3-coder-next
# this one is 8bit
#spring.ai.openai.chat.options.model=qwen3-coder-30b-a3b-instruct-mlx
# this one is 6bit
#spring.ai.openai.chat.options.model=qwen3-coder-30b-a3b-instruct-mlx-6
# this one is 4bit
#spring.ai.openai.chat.options.model=qwen3-coder-30b-a3b-instruct-mlx@4bit


# https://huggingface.co/lmstudio-community/Qwen3-Coder-30B-A3B-Instruct-MLX-4bit
# large 480B parameter version of qwen3-coder#$
#spring.ai.openai.chat.options.model=qwen/qwen3-coder-480b

# general purpose model about 142G in size, use for Evals?
#spring.ai.openai.chat.options.model=qwen3-235b-a22b-2507

# glm-4.7: large model, 290G size, 6 bit quantization, very slow
# spring.ai.openai.chat.options.model=glm-4.7

# nemotron-3-nano: good fast model!
#spring.ai.openai.chat.options.model=nvidia/nemotron-3-nano

# glm-4.7-flash: good fast model!
#spring.ai.openai.chat.options.model=glm-4.7-flash
#spring.ai.openai.chat.options.max-tokens=900000
dev.prpatel.evaluation.model=bespoke-minicheck

spring.main.allow-bean-definition-overriding=true
